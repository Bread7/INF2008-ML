# -*- coding: utf-8 -*-
"""Week03 Lab Student.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wxC5k8I2mNZCmg6wi-d0Fx1hweVTcsBI

## Week 03 INF2008 Lab: Linear Regression Case Study

We import some libaries below.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

"""Upload the csv data file into the Google colab file system.

Create a dataframe (dataset) by reading the CSV file: 'resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv'
"""

url = 'https://raw.githubusercontent.com/Bread7/INF2008-ML/main/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv'
df = pd.read_csv(url)

"""View the top 5 and bottom 5 rows."""

combine = pd.concat([df.head(), df.tail()])
combine

"""View only the top five rows."""

df.head()

"""View only the bottom five rows."""

df.tail()

"""View the columns in the dataframe."""

df.columns

"""Check the data types of the columns (hint, use the dtypes property)."""

df.dtypes

"""Check if there is any null or missing data in the dataframe."""

df.isnull()

"""Use the describe function to find out what are some of the basic statistical metrics in the numberical columns of the dataset."""

df.describe()

"""This describes the dataset for all the columns. It is useful to find number of  unique values for individual columns."""

include = ['object', 'float', 'int']
df.describe(include=include)

# Show correlation heat map and matrix
corrmat = df.corr()
sns.heatmap(corrmat,annot = True)
plt.show()

"""# **Data Preparation**

**Descide columns to drop / modify / categorise**

Columns to drop

#### Analyze Street name and lease commence date

For this exercise, we will be dropping the street name. However in your own analysis, you may want to consider using the street name as one of the x variables as well. Especially the distance from the address to the nearest MRT, or Primary school.

Drop also the lease commence date for this anaylsis. It doesn't cause any harm if you keep it. Just that it is another column to engineer and we want to keep things a bit simpler for this lab.

Drop columns "street_name", 'block' and "lease_commence_data".
"""

df.drop(['street_name','lease_commence_date', 'block'], axis=1, inplace=True)
df.head(5)

"""Typically we may want to check the numerical data for outliers but we are ignoring those for now.

#### Analyze month.

This is the resale registration date in YYYY-MM format.

Example: 2017-01

For simplicity, ignore the month part and copy only the year part to a new column 'reg_year'. After that remove the month column.
"""

new_df = df.assign(reg_year=df["month"].str[:4])
new_df

"""Drop month column."""

df2 = new_df.drop(columns='month')
df2

"""Split 'remaining_lease' to 'remain_lease_year' and 'remain_lease_month'"""

df3 = df2.assign(remain_lease_year=df2['remaining_lease'].str[:2])
df3 = df3.assign(remain_lease_month=df3['remaining_lease'].str[-9:])
df3

"""remove columns: 'remaining_lease' and 'remain_lease_month'"""

df3.drop(columns=['remain_lease_year', 'remain_lease_month'])

"""We now need to fix a lot of columns that are non numeric. From the dataframe above, they are:
*   town
*   flat_type
*   block
*   storey_range
*   flat_model

View categories in 'flat_type'
"""

df3['flat_type'].unique()

"""View categories in 'town' and count the number of unique categories in town."""

df3['town'].unique()
df3['town'].unique().size

"""As this is a lab on linear regression, non ordinal categorical variables do not work very well. Although town and flat_model has has an ordinal arrangement, you need to find out the order from other data sources. I will leave you to carry out this step. For now, we will remove the flat_model, town and flat_type.

*   flat_type
*   flat_model
*   town

"""

#df3 = df3.drop(columns=['town', 'flat_type', 'flat_model', 'remaining_lease', 'remain_lease_month'])
df3 = df3.drop(columns=['remaining_lease', 'remain_lease_month'])
df33

"""** Ordinal data have to be ranked and not categorized**

The last is storey_range. We will carry out some label encoding and check that the encoding done is correct.
"""

storey_range_unique = df['storey_range'].unique()
storey_range_unique

"""Check the ordering of the encoded values are correct."""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
mapped = le.fit_transform(storey_range_unique)

mapping = dict(zip(mapped, storey_range_unique))
sorted_dict = dict(sorted(mapping.items()))
print(sorted_dict)

df3['storey_range_cat'] = le.transform(df3['storey_range'])
df3.head(5)

"""Drop storey range column."""

df3 = df3.drop(columns='storey_range')
df3

"""Show the correlation mapping of the dataframe."""

# Show correlation heat map and matrix
corrmat = df3.corr()
sns.heatmap(corrmat,annot = True)
plt.show()

"""If two columns are highly correlated, drop one (probably the category column)

**Model the data**

Identify x and y variables.
"""

# X: Input columns
# y: Target column

feature_cols = ['floor_area_sqm','storey_range_cat', 'reg_year', 'remain_lease_year']

X = df3[feature_cols]
y = df3[['resale_price']].copy()

# print columns in X
X.columns

# Print y column name
y.columns

"""Split dataset for train and test

Import random and set a random seed of 15
"""

import random
random.seed(15)

"""Import library for splitting the dataset"""

from sklearn.model_selection import train_test_split

"""Split the data into training data (70%) and testing data (30%)."""

x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.3)
x_test

"""Modelling / Learning

Import the linear model from sklearn.
"""

from sklearn.linear_model import LinearRegression

"""Train the model."""

regr = LinearRegression()
regr

"""Test it on X_test and print out the first 20 values from the predicted y variables."""

regr.fit(x_train,y_train)
output = regr.predict(x_test)
output[:20]

"""Evaluate: Performance Metrics

Print out the coefficients and the y intercept.
"""

print(regr.coef_)
print(regr.intercept_)

"""# Multilinear Regression Equation

Import the metrics library from sklearn.
"""

from sklearn.metrics import mean_squared_error

"""Calculate the RMSE. You should get 102774"""

np.sqrt(mean_squared_error(y_test, output))

sns.regplot(x= y_test, y=output, line_kws={"color": "red"})

